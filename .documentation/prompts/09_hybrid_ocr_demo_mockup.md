## ðŸš€ Gradio Tag Glossary Mock UI

ðŸŽ¯ **Goal**: Build a simple Gradio app to simulate human-in-the-loop tag glossary generation using a single annotated region on a P&ID diagram image.

ðŸ“‚ **Directory**: `hybrid_ocr_gpt/`

ðŸ–¼ï¸ **Input Image**: Assume `diagram.png` is a pre-annotated image with a box around the tag `T1`.

ðŸ§ª **Mock Data**:
- Extracted Tag: `"T1"`
- GPT Suggested Description: `"Room 3 temperature sensor (Â°C) - monitors process area conditions."`

ðŸ“‹ **UI Requirements**:
- Display the image (`diagram.png`)
- Below the image, show:
  - Detected tag (`T1`)
  - Suggested description (editable `Textbox`)
  - Button: âœ… Accept or ðŸ“ Modify
- When accepted or modified, display a confirmation message and log the final decision to the console (for now).

ðŸ§  You can simulate this with Gradio + Python. Save the script in `hybrid_ocr_gpt/tag_reviewer.py`.

ðŸ‘©â€ðŸ’» **Example Interaction**:
> User sees `T1` on diagram with the prompt:  
> "Room 3 temperature sensor (Â°C)...", and edits it to  
> "Temperature probe inside Room 3 process chamber."

ðŸ“Ž Use this as a design mock for how GPT + OCR might be paired with a human glossary reviewer.

Build this as a fully working Gradio demo with a single tag â€” no backend logic or OCR needed for now.